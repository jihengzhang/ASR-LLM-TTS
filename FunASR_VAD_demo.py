import os
import time
import wave
import threading
import queue
import numpy as np
import pyaudio
from datetime import datetime
import colorama
from colorama import Fore, Back, Style

# åˆå§‹åŒ– colorama ä»¥æ”¯æŒé¢œè‰²è¾“å‡º
colorama.init()

try:
    from funasr import AutoModel
    FUNASR_AVAILABLE = True
    print(f"{Fore.GREEN}[æˆåŠŸ]{Style.RESET_ALL} FunASR æ¨¡å—å·²åŠ è½½")
except ImportError:
    FUNASR_AVAILABLE = False
    print(f"{Fore.RED}[è­¦å‘Š]{Style.RESET_ALL} FunASR æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨åŸºæœ¬çš„æŒ¯å¹…æ£€æµ‹")

class KeywordActivatedRecorder:
    def __init__(self, sample_rate=16000, chunk_size=1024, channels=1, 
                 format=pyaudio.paInt16, threshold=0.03, 
                 silence_duration=2.0, min_speech_duration=1.0,
                 buffer_duration=5.0):
        """
        åˆå§‹åŒ–å½•éŸ³å™¨
        
        å‚æ•°:
            sample_rate: é‡‡æ ·ç‡ (Hz)
            chunk_size: æ¯æ¬¡å¤„ç†çš„éŸ³é¢‘å—å¤§å°
            channels: é€šé“æ•°
            format: éŸ³é¢‘æ ¼å¼
            threshold: éŸ³é¢‘æ¿€æ´»é˜ˆå€¼ï¼ˆç”¨äºå¤‡ç”¨æ£€æµ‹ï¼‰
            silence_duration: åœæ­¢å½•éŸ³å‰çš„é™éŸ³æŒç»­æ—¶é—´ (ç§’)
            min_speech_duration: æœ€å°è¯­éŸ³æŒç»­æ—¶é—´ä»¥è¢«è®¤ä¸ºæ˜¯æœ‰æ•ˆè¯­éŸ³ (ç§’)
            buffer_duration: é¢„ç¼“å†²æŒç»­æ—¶é—´ (ç§’)ï¼Œç”¨äºæ•è·å…³é”®è¯ä¹‹å‰çš„éŸ³é¢‘
        """
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.channels = channels
        self.format = format
        self.threshold = threshold
        self.silence_duration = silence_duration
        self.min_speech_duration = min_speech_duration
        
        # è®¡ç®—é¢„ç¼“å†²åŒºå¤§å°
        self.buffer_frames = int(buffer_duration * sample_rate / chunk_size)
        self.buffer = queue.Queue(maxsize=self.buffer_frames)
        
        # å½•éŸ³çŠ¶æ€
        self.recording = False
        self.speech_detected = False
        self.silence_counter = 0
        self.speech_counter = 0
        self.max_silence_frames = int(silence_duration * sample_rate / chunk_size)
        self.min_speech_frames = int(min_speech_duration * sample_rate / chunk_size)
        
        # éŸ³é¢‘å¯¹è±¡
        self.pyaudio = pyaudio.PyAudio()
        self.stream = None
        self.frames = []
        
        # æ£€æŸ¥éŸ³é¢‘è®¾å¤‡
        self.check_audio_devices()
        
        # çº¿ç¨‹é”
        self.lock = threading.Lock()
        
        # FunASR æ¨¡å‹åˆå§‹åŒ–
        self.vad_model = None
        self.asr_model = None
        self.use_funasr = FUNASR_AVAILABLE
        
        if self.use_funasr:
            self.init_funasr_models()
        else:
            print(f"{Fore.YELLOW}[ä¿¡æ¯]{Style.RESET_ALL} ä½¿ç”¨åŸºæœ¬æŒ¯å¹…æ£€æµ‹ç®—æ³•")
        
        # å…³é”®è¯æ£€æµ‹ç›¸å…³å˜é‡
        self.keyword_buffer = queue.Queue(maxsize=int(4.0 * sample_rate / chunk_size))  # 4ç§’å…³é”®è¯ç¼“å†²
        self.keyword_detected = False
        self.speech_segments = []  # å­˜å‚¨æ£€æµ‹åˆ°çš„è¯­éŸ³æ®µ
        self.keyword_candidates = ["ä½ å¥½", "å°åŠ©æ‰‹", "å¼€å§‹å½•éŸ³", "å½•éŸ³å¼€å§‹", "å°çˆ±", "å°åº¦"]
        self.current_keyword = "ä½ å¥½"  # é»˜è®¤å…³é”®è¯
        
        # VAD ç›¸å…³å˜é‡
        self.vad_cache = {}
        self.accumulated_audio = np.array([], dtype=np.float32)
        self.last_vad_check = time.time()
        self.vad_check_interval = 3.0  # æ¯3ç§’è¿›è¡Œä¸€æ¬¡VADæ£€æŸ¥
        # ä½¿ç”¨16000é‡‡æ ·ç‚¹ï¼ˆ1ç§’ï¼‰çš„çª—å£ï¼Œç¡®ä¿æ˜¯400çš„å€æ•°
        self.vad_window_size = 16000  # 1ç§’çª—å£
        self.audio_buffer_for_vad = np.array([], dtype=np.float32)
        
        print(f"{Fore.YELLOW}[å…³é”®è¯]{Style.RESET_ALL} å½“å‰æ¿€æ´»å…³é”®è¯: '{self.current_keyword}'")
        print(f"{Fore.CYAN}[æç¤º]{Style.RESET_ALL} ä½¿ç”¨ FunASR VAD+ASR è¿›è¡Œå…³é”®è¯æ£€æµ‹")
    
    def init_funasr_models(self):
        """åˆå§‹åŒ– FunASR VAD å’Œ ASR æ¨¡å‹"""
        try:
            print(f"{Fore.CYAN}[åˆå§‹åŒ–]{Style.RESET_ALL} æ­£åœ¨åŠ è½½ FunASR VAD æ¨¡å‹...")
            # åŠ è½½ VAD æ¨¡å‹
            self.vad_model = AutoModel(
                model="fsmn-vad",
                model_revision="v2.0.4"
            )
            print(f"{Fore.GREEN}[æˆåŠŸ]{Style.RESET_ALL} FunASR VAD æ¨¡å‹åŠ è½½å®Œæˆ")
            
            print(f"{Fore.CYAN}[åˆå§‹åŒ–]{Style.RESET_ALL} æ­£åœ¨åŠ è½½ FunASR ASR æ¨¡å‹...")
            # åŠ è½½ ASR æ¨¡å‹ç”¨äºå…³é”®è¯è¯†åˆ«
            self.asr_model = AutoModel(
                model="paraformer-zh",
                model_revision="v2.0.4"
            )
            print(f"{Fore.GREEN}[æˆåŠŸ]{Style.RESET_ALL} FunASR ASR æ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"{Fore.RED}[é”™è¯¯]{Style.RESET_ALL} FunASR æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.use_funasr = False
            self.vad_model = None
            self.asr_model = None

    def start(self):
        """å¼€å§‹ç›‘å¬éŸ³é¢‘"""
        print(f"{Fore.CYAN}[ç³»ç»Ÿ]{Style.RESET_ALL} å¯åŠ¨éŸ³é¢‘ç›‘å¬...")
        self.stream = self.pyaudio.open(
            format=self.format,
            channels=self.channels,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk_size,
            stream_callback=self.audio_callback
        )
        self.stream.start_stream()
        print(f"{Fore.GREEN}[å°±ç»ª]{Style.RESET_ALL} è¯­éŸ³æ¿€æ´»ç³»ç»Ÿå·²å¯åŠ¨ï¼Œæ­£åœ¨ç­‰å¾…è¯­éŸ³...")
        
    def stop(self):
        """åœæ­¢ç›‘å¬å¹¶å…³é—­èµ„æº"""
        print(f"{Fore.CYAN}[ç³»ç»Ÿ]{Style.RESET_ALL} æ­£åœ¨å…³é—­éŸ³é¢‘ç³»ç»Ÿ...")
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        self.pyaudio.terminate()
        print(f"{Fore.GREEN}[å®Œæˆ]{Style.RESET_ALL} éŸ³é¢‘ç³»ç»Ÿå·²å…³é—­")
    
    def audio_callback(self, in_data, frame_count, time_info, status):
        """éŸ³é¢‘å›è°ƒå‡½æ•°ï¼Œå¤„ç†è¾“å…¥çš„éŸ³é¢‘æ•°æ®"""
        # å°†äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ä¸ºæ•°ç»„
        audio_data = np.frombuffer(in_data, dtype=np.int16)
        
        # è®¡ç®—éŸ³é¢‘æŒ¯å¹… (å½’ä¸€åŒ–)
        amplitude = np.abs(audio_data).mean() / 32767.0
        
        # æ·»åŠ åˆ°é¢„ç¼“å†²åŒº
        try:
            if self.buffer.full():
                self.buffer.get_nowait()
            self.buffer.put_nowait(in_data)
        except queue.Full:
            pass
        
        # VAD æ£€æµ‹
        is_speech = amplitude > self.threshold  # é»˜è®¤ä½¿ç”¨æŒ¯å¹…
        
        # å¦‚æœ FunASR VAD å¯ç”¨ï¼Œä½¿ç”¨å®ƒè¿›è¡Œæ£€æµ‹
        if self.vad_model is not None:
            try:
                # å°†äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ä¸º float32 å¹¶å½’ä¸€åŒ–
                audio_float = audio_data.astype(np.float32) / 32768.0
                
                # æ·»åŠ åˆ°VADä¸“ç”¨ç¼“å†²åŒº
                self.audio_buffer_for_vad = np.concatenate([self.audio_buffer_for_vad, audio_float])
                
                # å®šæœŸè¿›è¡Œ VAD æ£€æµ‹
                current_time = time.time()
                if current_time - self.last_vad_check >= self.vad_check_interval and not self.recording:
                    self.last_vad_check = current_time
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„éŸ³é¢‘æ•°æ®è¿›è¡ŒVAD
                    if len(self.audio_buffer_for_vad) >= self.vad_window_size:
                        # ç¡®ä¿çª—å£å¤§å°æ˜¯400çš„ç²¾ç¡®å€æ•°
                        window_samples = (self.vad_window_size // 400) * 400
                        vad_window = self.audio_buffer_for_vad[:window_samples]
                        
                        try:
                            # ä½¿ç”¨ VAD æ¨¡å‹è¿›è¡Œè¯­éŸ³æ£€æµ‹
                            vad_result = self.vad_model.generate(
                                input=vad_window.reshape(1, -1),
                                chunk_size=400  # æŒ‡å®šåˆ†å—å¤§å°
                            )
                            
                            # æ£€æŸ¥ VAD ç»“æœ
                            if isinstance(vad_result, list) and len(vad_result) > 0:
                                result_dict = vad_result[0]
                                if isinstance(result_dict, dict) and 'value' in result_dict:
                                    vad_segments = result_dict['value']
                                    
                                    # å¦‚æœæ£€æµ‹åˆ°è¯­éŸ³æ®µï¼Œå¤„ç†å®ƒä»¬
                                    if vad_segments and not self.recording and not self.keyword_detected:
                                        print(f"\n{Fore.CYAN}[VAD]{Style.RESET_ALL} æ£€æµ‹åˆ° {len(vad_segments)} ä¸ªè¯­éŸ³æ®µ")
                                        
                                        # é’ˆå¯¹æ¯ä¸ªæ£€æµ‹åˆ°çš„è¯­éŸ³æ®µæå–éŸ³é¢‘
                                        for segment in vad_segments:
                                            if isinstance(segment, (list, tuple)) and len(segment) >= 2:
                                                start_time, end_time = segment[0], segment[1]
                                                # æ—¶é—´å•ä½è½¬æ¢ä¸ºé‡‡æ ·ç‚¹
                                                start_sample = int(start_time * self.sample_rate)
                                                end_sample = int(end_time * self.sample_rate)
                                                
                                                # æ£€æŸ¥ç´¢å¼•èŒƒå›´
                                                if start_sample < 0:
                                                    start_sample = 0
                                                if end_sample > len(vad_window):
                                                    end_sample = len(vad_window)
                                                
                                                # æå–éŸ³é¢‘æ®µå¹¶è¿›è¡Œå…³é”®è¯è¯†åˆ«
                                                if end_sample > start_sample:
                                                    segment_audio = vad_window[start_sample:end_sample]
                                                    if len(segment_audio) >= 1600:  # è‡³å°‘100ms
                                                        # ä½¿ç”¨çº¿ç¨‹è¿›è¡Œå¼‚æ­¥å…³é”®è¯è¯†åˆ«
                                                        threading.Thread(
                                                            target=self.recognize_keyword,
                                                            args=(segment_audio,),
                                                            daemon=True
                                                        ).start()
                                                        
                                                        # æ£€æŸ¥æŒ¯å¹…ï¼Œå¦‚æœè¶³å¤Ÿé«˜ä¹Ÿè®¤ä¸ºæ˜¯è¯­éŸ³
                                                        segment_amplitude = np.abs(segment_audio).mean()
                                                        if segment_amplitude > self.threshold * 1.5:
                                                            is_speech = True
                                            else:
                                                print(f"\n{Fore.YELLOW}[VAD]{Style.RESET_ALL} æ— æ•ˆè¯­éŸ³æ®µæ ¼å¼: {segment}")
                        except Exception as vad_error:
                            print(f"\n{Fore.RED}[VADå¤„ç†é”™è¯¯]{Style.RESET_ALL} {vad_error}")
                            # å¦‚æœVADå¤„ç†å¤±è´¥ï¼Œé€€å›åˆ°åŸºäºæŒ¯å¹…çš„æ£€æµ‹
                            is_speech = amplitude > self.threshold
                            
                            # å¦‚æœæŒ¯å¹…è¶³å¤Ÿå¤§ï¼Œä¹Ÿå°è¯•ç›´æ¥ç”¨ASR
                            if amplitude > 0.03 and not self.recording and not self.keyword_detected:
                                # ç”¨æœ€è¿‘çš„ä¸€æ®µéŸ³é¢‘ç›´æ¥å°è¯•ASRå…³é”®è¯è¯†åˆ«
                                recent_audio = self.audio_buffer_for_vad[-16000:] if len(self.audio_buffer_for_vad) > 16000 else self.audio_buffer_for_vad
                                if len(recent_audio) >= 8000:  # è‡³å°‘0.5ç§’
                                    threading.Thread(
                                        target=self.recognize_keyword,
                                        args=(recent_audio,),
                                        daemon=True
                                    ).start()
                        
                        # ä¿ç•™ä¸€éƒ¨åˆ†éŸ³é¢‘ä½œä¸ºä¸‹æ¬¡æ£€æµ‹çš„ä¸Šä¸‹æ–‡
                        overlap_size = window_samples // 3  # ä¿ç•™33%çš„é‡å 
                        self.audio_buffer_for_vad = self.audio_buffer_for_vad[-overlap_size:]
                    
                    # é™åˆ¶ç¼“å†²åŒºå¤§å°ï¼Œé˜²æ­¢å†…å­˜è¿‡å¤§
                    max_buffer_size = self.vad_window_size * 2  # æœ€å¤šä¿ç•™2ä¸ªçª—å£çš„æ•°æ®
                    if len(self.audio_buffer_for_vad) > max_buffer_size:
                        self.audio_buffer_for_vad = self.audio_buffer_for_vad[-max_buffer_size:]
                        
            except Exception as e:
                print(f"\n{Fore.RED}[VADé”™è¯¯]{Style.RESET_ALL} {e}")
                # å›é€€åˆ°æŒ¯å¹…æ£€æµ‹
                is_speech = amplitude > self.threshold
        else:
            # å¦‚æœ FunASR ä¸å¯ç”¨ï¼Œä½¿ç”¨æŒ¯å¹…æ£€æµ‹
            is_speech = amplitude > self.threshold
        
        # æ˜¾ç¤ºéŸ³é¢‘ç”µå¹³å’ŒçŠ¶æ€
        vad_status = "ğŸ¤ è¯­éŸ³" if is_speech else "ğŸ”‡ é™éŸ³"
        vad_color = Fore.GREEN if is_speech else Fore.BLUE
        
        if amplitude > 0.001:  # åªåœ¨æœ‰éŸ³é¢‘è¾“å…¥æ—¶æ˜¾ç¤º
            level_bars = int(amplitude * 30)
            level_display = 'â–ˆ' * level_bars + 'â–‘' * (30 - level_bars)
            status_line = f"{vad_color}{vad_status}{Style.RESET_ALL} | ç”µå¹³: [{level_display}] {amplitude:.3f}"
            
            # æ˜¾ç¤ºå½•éŸ³çŠ¶æ€
            if self.recording:
                status_line += f" | {Fore.RED}â— å½•éŸ³ä¸­{Style.RESET_ALL}"
            elif self.keyword_detected:
                status_line += f" | {Fore.YELLOW}â— å…³é”®è¯æ¿€æ´»: {self.current_keyword}{Style.RESET_ALL}"
            
            print(f"\r{status_line}", end='', flush=True)
        
        # çŠ¶æ€é€»è¾‘å¤„ç†
        with self.lock:
            if is_speech:
                self.speech_counter += 1
                self.silence_counter = 0
                
                # å…³é”®è¯æ¿€æ´»åå¼€å§‹å½•éŸ³ - å¼ºåŒ–æ¿€æ´»é€»è¾‘
                if self.keyword_detected and not self.recording:
                    if self.speech_counter >= self.min_speech_frames:
                        print(f"\n{Fore.GREEN}[å½•éŸ³]{Style.RESET_ALL} å…³é”®è¯ '{self.current_keyword}' æ¿€æ´» - å¼€å§‹å½•éŸ³")
                        self.start_recording()
                    else:
                        print(f"\r{Fore.YELLOW}[ç­‰å¾…]{Style.RESET_ALL} ç­‰å¾…è¶³å¤Ÿçš„è¯­éŸ³æŒç»­æ—¶é—´: {self.speech_counter}/{self.min_speech_frames}", end='', flush=True)
            else:
                self.silence_counter += 1
                
                # å¦‚æœé™éŸ³è¶³å¤Ÿé•¿å¹¶ä¸”æ­£åœ¨å½•éŸ³ï¼Œåˆ™åœæ­¢å½•éŸ³
                if self.silence_counter >= self.max_silence_frames and self.recording:
                    self.stop_recording()
                    
                # å¦‚æœè¯­éŸ³ä¸å¤Ÿé•¿ï¼Œé‡ç½®è¯­éŸ³è®¡æ•°å™¨
                if not self.recording and self.speech_counter < self.min_speech_frames:
                    self.speech_counter = 0
        
        return (in_data, pyaudio.paContinue)
    
    def process_vad_segments(self, vad_segments, audio_data):
        """å¤„ç† VAD æ£€æµ‹åˆ°çš„è¯­éŸ³æ®µï¼ˆå·²ç®€åŒ–ï¼Œä¸»è¦ç”±recognize_keywordå¤„ç†ï¼‰"""
        # è¿™ä¸ªæ–¹æ³•ç°åœ¨ä¸»è¦ç”¨äºè°ƒè¯•ä¿¡æ¯
        for segment in vad_segments:
            try:
                if isinstance(segment, (list, tuple)) and len(segment) >= 2:
                    start_time, end_time = segment[0], segment[1]
                    print(f"\n{Fore.CYAN}[VADæ®µ]{Style.RESET_ALL} æ—¶é—´: {start_time:.2f}s - {end_time:.2f}s")
            except Exception as e:
                print(f"\n{Fore.YELLOW}[VADæ®µè­¦å‘Š]{Style.RESET_ALL} å¤„ç†æ®µä¿¡æ¯æ—¶å‡ºé”™: {e}")

    def recognize_keyword(self, audio_segment):
        """ä½¿ç”¨ ASR è¯†åˆ«è¯­éŸ³æ®µä¸­çš„å…³é”®è¯"""
        if self.asr_model is None or self.keyword_detected or self.recording:
            return
            
        try:
            # ç¡®ä¿éŸ³é¢‘æ®µæ ¼å¼æ­£ç¡®
            if len(audio_segment.shape) == 1:
                audio_input = audio_segment.reshape(1, -1)
            else:
                audio_input = audio_segment
            
            # ä½¿ç”¨ ASR æ¨¡å‹è¿›è¡Œè¯­éŸ³è¯†åˆ«
            asr_result = self.asr_model.generate(
                input=audio_input,
                cache={},
                language="zh"
            )
            
            # è§£æè¯†åˆ«ç»“æœ
            if isinstance(asr_result, list) and len(asr_result) > 0:
                result_dict = asr_result[0]
                if isinstance(result_dict, dict):
                    recognized_text = result_dict.get('text', '').strip()
                    
                    if recognized_text:
                        print(f"\n{Fore.BLUE}[è¯†åˆ«]{Style.RESET_ALL} è¯­éŸ³è¯†åˆ«ç»“æœ: '{recognized_text}'")
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                        for keyword in self.keyword_candidates:
                            if keyword in recognized_text:
                                with self.lock:
                                    if not self.keyword_detected and not self.recording:
                                        print(f"\n{Fore.GREEN}[å…³é”®è¯]{Style.RESET_ALL} æ£€æµ‹åˆ°å…³é”®è¯ '{keyword}' - å·²æ¿€æ´»")
                                        self.keyword_detected = True
                                        self.current_keyword = keyword
                                        self.speech_counter = max(self.speech_counter, self.min_speech_frames // 2)  # åŠ å¿«æ¿€æ´»é€Ÿåº¦
                                        return
                    else:
                        print(f"\n{Fore.YELLOW}[ASR]{Style.RESET_ALL} è¯†åˆ«ç»“æœä¸ºç©º")
            
        except Exception as e:
            print(f"\n{Fore.RED}[ASRé”™è¯¯]{Style.RESET_ALL} è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
    
    def start_recording(self):
        """å¼€å§‹å½•éŸ³"""
        with self.lock:
            if not self.recording:
                self.recording = True
                self.frames = []
                
                # æ·»åŠ é¢„ç¼“å†²åŒºä¸­çš„æ‰€æœ‰å¸§
                while not self.buffer.empty():
                    self.frames.append(self.buffer.get())
                
                print(f"\n{Fore.GREEN}[å½•éŸ³]{Style.RESET_ALL} å…³é”®è¯æ¿€æ´»æˆåŠŸ - å¼€å§‹å½•éŸ³...")
                print(f"{Fore.CYAN}[çŠ¶æ€]{Style.RESET_ALL} è¯·ç»§ç»­è¯´è¯ï¼Œé™éŸ³ {self.silence_duration} ç§’åè‡ªåŠ¨åœæ­¢")
                
                # é‡ç½®è®¡æ•°å™¨å’ŒçŠ¶æ€
                self.silence_counter = 0
                # æ¸…ç©ºå…³é”®è¯ç¼“å†²åŒºï¼Œä¸ºä¸‹æ¬¡æ£€æµ‹åšå‡†å¤‡
                while not self.keyword_buffer.empty():
                    try:
                        self.keyword_buffer.get_nowait()
                    except queue.Empty:
                        break
    
    def stop_recording(self):
        """åœæ­¢å½•éŸ³å¹¶ä¿å­˜æ–‡ä»¶"""
        with self.lock:
            if self.recording:
                self.recording = False
                print(f"\n{Fore.YELLOW}[å½•éŸ³]{Style.RESET_ALL} æ£€æµ‹åˆ°é™éŸ³ - åœæ­¢å½•éŸ³")
                
                # å¦‚æœæœ‰è¶³å¤Ÿçš„å¸§ï¼Œä¿å­˜å½•éŸ³
                if len(self.frames) > 0:
                    threading.Thread(target=self.save_recording).start()
                
                # é‡ç½®è®¡æ•°å™¨å’ŒçŠ¶æ€
                self.speech_counter = 0
                self.speech_detected = False
                self.keyword_detected = False  # é‡ç½®å…³é”®è¯çŠ¶æ€ï¼Œå‡†å¤‡ä¸‹æ¬¡æ£€æµ‹
                
                print(f"{Fore.CYAN}[ç­‰å¾…]{Style.RESET_ALL} ç­‰å¾…ä¸‹ä¸€ä¸ªå…³é”®è¯ '{self.current_keyword}'...")
    
    def save_recording(self):
        """å°†å½•éŸ³ä¿å­˜ä¸º WAV æ–‡ä»¶"""
        if not self.frames:
            print(f"{Fore.RED}[é”™è¯¯]{Style.RESET_ALL} æ²¡æœ‰å½•éŸ³æ•°æ®å¯ä¿å­˜")
            return
            
        # ç¡®ä¿å­˜åœ¨å½•éŸ³ç›®å½•
        if not os.path.exists("recordings"):
            os.makedirs("recordings")
            
        # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = os.path.join("recordings", f"recording_{timestamp}.wav")
        
        print(f"{Fore.BLUE}[ä¿å­˜]{Style.RESET_ALL} æ­£åœ¨ä¿å­˜å½•éŸ³åˆ°: {filename}")
        
        # ä¿å­˜ä¸º WAV æ–‡ä»¶
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(self.channels)
            wf.setsampwidth(self.pyaudio.get_sample_size(self.format))
            wf.setframerate(self.sample_rate)
            wf.writeframes(b''.join(self.frames))
             
        duration = len(self.frames) * self.chunk_size / self.sample_rate
        file_size = os.path.getsize(filename) / 1024  # KB
        
        print(f"{Fore.GREEN}[å®Œæˆ]{Style.RESET_ALL} å½•éŸ³å·²ä¿å­˜! æ–‡ä»¶: {filename}")
        print(f"{Fore.CYAN}[ä¿¡æ¯]{Style.RESET_ALL} å½•éŸ³æ—¶é•¿: {duration:.2f} ç§’, æ–‡ä»¶å¤§å°: {file_size:.2f} KB")
        
        # æ¸…ç©ºå¸§
        self.frames = []

    def process_chunk(self, in_data):
        """å¤„ç†å•ä¸ªéŸ³é¢‘å—ï¼ˆç”¨äºéå›è°ƒæ¨¡å¼ï¼‰"""
        if self.recording:
            self.frames.append(in_data)
    
    def check_audio_devices(self):
        """æ£€æŸ¥å¯ç”¨çš„éŸ³é¢‘è®¾å¤‡"""
        device_count = self.pyaudio.get_device_count()
        print(f"{Fore.BLUE}[è®¾å¤‡]{Style.RESET_ALL} æ£€æµ‹åˆ° {device_count} ä¸ªéŸ³é¢‘è®¾å¤‡:")
        
        input_devices = []
        for i in range(device_count):
            try:
                info = self.pyaudio.get_device_info_by_index(i)
                if info['maxInputChannels'] > 0:
                    input_devices.append((i, info['name']))
                    print(f"  {Fore.GREEN}[{i}]{Style.RESET_ALL} {info['name']} (è¾“å…¥é€šé“: {info['maxInputChannels']})")
            except Exception as e:
                continue
        
        if not input_devices:
            raise Exception("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡")
        
        try:
            default_input = self.pyaudio.get_default_input_device_info()
            print(f"{Fore.GREEN}[é»˜è®¤]{Style.RESET_ALL} ä½¿ç”¨é»˜è®¤è¾“å…¥è®¾å¤‡: {default_input['name']}")
        except Exception as e:
            print(f"{Fore.YELLOW}[è­¦å‘Š]{Style.RESET_ALL} æ— æ³•è·å–é»˜è®¤è¾“å…¥è®¾å¤‡: {e}")
        
        print(f"{Fore.CYAN}[å°±ç»ª]{Style.RESET_ALL} æ‰¾åˆ° {len(input_devices)} ä¸ªå¯ç”¨è¾“å…¥è®¾å¤‡\n")
    
    def detect_keyword_in_audio(self, audio_data):
        """
        å¤‡ç”¨çš„å…³é”®è¯æ£€æµ‹ç®—æ³•ï¼ˆå½“ FunASR ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
        """
        try:
            # è®¡ç®—éŸ³é¢‘ç‰¹å¾
            amplitude = np.abs(audio_data).mean()
            audio_length = len(audio_data) / self.sample_rate
            
            # ç®€åŒ–çš„å¯å‘å¼æ£€æµ‹
            if 0.4 <= audio_length <= 2.0 and amplitude > self.threshold * 2:
                mid_point = len(audio_data) // 2
                first_half_energy = np.sum(audio_data[:mid_point].astype(float) ** 2)
                second_half_energy = np.sum(audio_data[mid_point:].astype(float) ** 2)
                
                energy_ratio = min(first_half_energy, second_half_energy) / max(first_half_energy, second_half_energy)
                
                if energy_ratio > 0.3:
                    return True, f"æ£€æµ‹åˆ°å¯èƒ½çš„å…³é”®è¯ (æ—¶é•¿:{audio_length:.2f}s, èƒ½é‡æ¯”:{energy_ratio:.2f})"
            
            return False, f"éŸ³é¢‘ä¸åŒ¹é…å…³é”®è¯æ¨¡å¼ (æ—¶é•¿:{audio_length:.2f}s, æŒ¯å¹…:{amplitude:.3f})"
            
        except Exception as e:
            return False, f"å…³é”®è¯æ£€æµ‹é”™è¯¯: {e}"

    def process_keyword_detection(self, audio_chunk):
        """å¤„ç†å…³é”®è¯æ£€æµ‹ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼Œå½“ FunASR ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
        if self.use_funasr:
            return False  # ä½¿ç”¨ FunASR æ—¶ä¸ä½¿ç”¨æ­¤æ–¹æ³•
            
        # å°†éŸ³é¢‘å—æ·»åŠ åˆ°å…³é”®è¯ç¼“å†²åŒº
        try:
            if self.keyword_buffer.full():
                self.keyword_buffer.get_nowait()
            
            # è½¬æ¢ä¸º numpy æ•°ç»„
            if isinstance(audio_chunk, bytes):
                audio_array = np.frombuffer(audio_chunk, dtype=np.int16)
            else:
                audio_array = audio_chunk
                
            self.keyword_buffer.put_nowait(audio_array)
        except queue.Full:
            pass
        
        # æ”¶é›†ç¼“å†²åŒºä¸­çš„éŸ³é¢‘ç”¨äºå…³é”®è¯æ£€æµ‹
        if not self.keyword_buffer.empty():
            buffer_data = []
            temp_queue = queue.Queue()
            
            while not self.keyword_buffer.empty():
                try:
                    chunk = self.keyword_buffer.get_nowait()
                    buffer_data.append(chunk)
                    temp_queue.put(chunk)
                except queue.Empty:
                    break
            
            # æ¢å¤é˜Ÿåˆ—çŠ¶æ€
            while not temp_queue.empty():
                try:
                    self.keyword_buffer.put_nowait(temp_queue.get_nowait())
                except queue.Full:
                    break
            
            # åˆå¹¶éŸ³é¢‘æ•°æ®è¿›è¡Œå…³é”®è¯æ£€æµ‹
            if len(buffer_data) > 5:
                combined_audio = np.concatenate(buffer_data)
                is_keyword, message = self.detect_keyword_in_audio(combined_audio)
                
                if is_keyword and not self.keyword_detected and not self.recording:
                    print(f"\n{Fore.GREEN}[å…³é”®è¯]{Style.RESET_ALL} {message}")
                    print(f"{Fore.GREEN}[æ¿€æ´»]{Style.RESET_ALL} æ£€æµ‹åˆ°å…³é”®è¯ '{self.current_keyword}' - å‡†å¤‡å¼€å§‹å½•éŸ³")
                    self.keyword_detected = True
                    return True
        
        return False

if __name__ == "__main__":
    print(f"\n{Fore.CYAN}{Style.BRIGHT}{'='*60}{Style.RESET_ALL}")
    print(f"{Fore.CYAN}{Style.BRIGHT}           FunASR VAD+ASR å…³é”®è¯æ¿€æ´»å½•éŸ³ç³»ç»Ÿ{Style.RESET_ALL}")
    print(f"{Fore.CYAN}{Style.BRIGHT}{'='*60}{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}å·¥ä½œæ¨¡å¼: FunASR VAD+ASR å…³é”®è¯æ¿€æ´»å½•éŸ³{Style.RESET_ALL}")
    print(f"  â€¢ é»˜è®¤å…³é”®è¯: {Fore.GREEN}'ä½ å¥½'{Style.RESET_ALL}")
    print(f"  â€¢ æ”¯æŒå…³é”®è¯: ä½ å¥½, å°åŠ©æ‰‹, å¼€å§‹å½•éŸ³, å½•éŸ³å¼€å§‹, å°çˆ±, å°åº¦")
    print(f"  â€¢ æ£€æµ‹æ–¹å¼: VADæ£€æµ‹è¯­éŸ³æ®µ + ASRè¯†åˆ«å…³é”®è¯")
    print(f"{Fore.YELLOW}ç³»ç»Ÿé…ç½®:{Style.RESET_ALL}")
    print(f"  â€¢ VAD æ£€æŸ¥é—´éš”: 3.0 ç§’")
    print(f"  â€¢ VAD çª—å£å¤§å°: 1.0ç§’ (16000 é‡‡æ ·ç‚¹)")
    print(f"  â€¢ é™éŸ³è¶…æ—¶: 2.0 ç§’")
    print(f"  â€¢ æœ€å°è¯­éŸ³æ—¶é•¿: 0.8 ç§’") 
    print(f"  â€¢ é¢„ç¼“å†²æ—¶é•¿: 5.0 ç§’")
    print(f"{Fore.RED}é‡è¦: ç³»ç»Ÿä½¿ç”¨å›ºå®š400åˆ†å—å¤§å°çš„VADæ£€æµ‹ï¼{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}æ§åˆ¶è¯´æ˜: æŒ‰ Ctrl+C é€€å‡ºç¨‹åº{Style.RESET_ALL}")
    print(f"{Fore.CYAN}{Style.BRIGHT}{'='*60}{Style.RESET_ALL}\n")
    
    # åˆ›å»ºå¹¶å¯åŠ¨å½•éŸ³å™¨
    try:
        recorder = KeywordActivatedRecorder(
            threshold=0.03,              # å¤‡ç”¨æŒ¯å¹…é˜ˆå€¼
            silence_duration=2.0,        # åœæ­¢å½•éŸ³å‰çš„é™éŸ³æŒç»­æ—¶é—´ (ç§’)
            min_speech_duration=0.5,     # æœ€å°è¯­éŸ³æŒç»­æ—¶é—´ä»¥ä¾¿å¼€å§‹å½•éŸ³ (ç§’)
            buffer_duration=5.0          # é¢„ç¼“å†²æŒç»­æ—¶é—´ (ç§’)
        )
        
        recorder.start()
        print(f"{Fore.GREEN}[çŠ¶æ€]{Style.RESET_ALL} FunASR VAD+ASR å…³é”®è¯æ£€æµ‹ç³»ç»Ÿå·²å¯åŠ¨")
        print(f"{Fore.CYAN}[ç­‰å¾…]{Style.RESET_ALL} è¯·è¯´å‡ºå…³é”®è¯ '{Fore.GREEN}ä½ å¥½{Style.RESET_ALL}' æ¥æ¿€æ´»å½•éŸ³")
        print(f"{Fore.BLUE}[æç¤º]{Style.RESET_ALL} ç³»ç»Ÿä¼šå®æ—¶æ˜¾ç¤ºVADæ£€æµ‹çŠ¶æ€å’Œè¯­éŸ³è¯†åˆ«ç»“æœ\n")
        
        # ä¿æŒç¨‹åºè¿è¡Œ
        while True:
            time.sleep(0.1)
            
    except KeyboardInterrupt:
        print(f"\n\n{Fore.YELLOW}[ç³»ç»Ÿ]{Style.RESET_ALL} æ£€æµ‹åˆ°é”®ç›˜ä¸­æ–­ï¼Œæ­£åœ¨å…³é—­...")
    except Exception as e:
        print(f"\n{Fore.RED}[é”™è¯¯]{Style.RESET_ALL} ç³»ç»Ÿé”™è¯¯: {e}")
    finally:
        try:
            recorder.stop()
        except:
            pass
        print(f"{Fore.GREEN}[å®Œæˆ]{Style.RESET_ALL} ç¨‹åºå·²å®‰å…¨é€€å‡º")
            
    except KeyboardInterrupt:
        print(f"\n\n{Fore.YELLOW}[ç³»ç»Ÿ]{Style.RESET_ALL} æ£€æµ‹åˆ°é”®ç›˜ä¸­æ–­ï¼Œæ­£åœ¨å…³é—­...")
    except Exception as e:
        print(f"\n{Fore.RED}[é”™è¯¯]{Style.RESET_ALL} ç³»ç»Ÿé”™è¯¯: {e}")
    finally:
        try:
            recorder.stop()
        except:
            pass
        print(f"{Fore.GREEN}[å®Œæˆ]{Style.RESET_ALL} ç¨‹åºå·²å®‰å…¨é€€å‡º")
