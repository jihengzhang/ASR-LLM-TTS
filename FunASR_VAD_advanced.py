#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
FunASR VAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹å½•éŸ³ç³»ç»Ÿ
å®ç°çœŸæ­£çš„è¯­éŸ³æ´»åŠ¨æ£€æµ‹åŠŸèƒ½
"""

import os
import time
import wave
import numpy as np
import threading
import queue
from datetime import datetime
import collections
import colorama
from colorama import Fore, Style
from scipy import signal
from scipy.fft import fft

# åˆå§‹åŒ– colorama ä»¥æ”¯æŒ Windows æ§åˆ¶å°é¢œè‰²
colorama.init()

try:
    import pyaudio
    PYAUDIO_AVAILABLE = True
except ImportError:
    print(f"{Fore.RED}PyAudio æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install pyaudio{Style.RESET_ALL}")
    PYAUDIO_AVAILABLE = False

try:
    from funasr import AutoModel
    FUNASR_AVAILABLE = True
    print(f"{Fore.GREEN}âœ“ FunASR æ¨¡å—å·²åŠ è½½{Style.RESET_ALL}")
except ImportError:
    FUNASR_AVAILABLE = False
    print(f"{Fore.YELLOW}âš  FunASR æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ä½¿ç”¨å†…ç½® VAD ç®—æ³•{Style.RESET_ALL}")

if not PYAUDIO_AVAILABLE:
    print(f"{Fore.RED}è¯·å…ˆå®‰è£…å¿…è¦çš„ä¾èµ–: pip install pyaudio scipy{Style.RESET_ALL}")
    exit(1)

# é…ç½®å‚æ•°
RATE = 16000  # é‡‡æ ·ç‡
CHUNK = 1024  # æ¯ä¸ªç¼“å†²åŒºçš„å¸§æ•°
FORMAT = pyaudio.paInt16  # éŸ³é¢‘æ ¼å¼
CHANNELS = 1  # å£°é“æ•°

# VAD å‚æ•°
VAD_FRAME_MS = 30  # VAD å¸§é•¿åº¦ (æ¯«ç§’)
VAD_FRAME_SIZE = int(RATE * VAD_FRAME_MS / 1000)  # VAD å¸§å¤§å°
ENERGY_THRESHOLD = 0.01  # èƒ½é‡é˜ˆå€¼
ZCR_THRESHOLD = 0.3  # è¿‡é›¶ç‡é˜ˆå€¼
SPECTRAL_CENTROID_THRESHOLD = 1000  # é¢‘è°±é‡å¿ƒé˜ˆå€¼
SILENCE_DURATION = 1.5  # é™éŸ³æŒç»­æ—¶é—´(ç§’)
MIN_SPEECH_DURATION = 0.5  # æœ€å°è¯­éŸ³æŒç»­æ—¶é—´(ç§’)

class VADProcessor:
    """è¯­éŸ³æ´»åŠ¨æ£€æµ‹å¤„ç†å™¨"""
    
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        self.frame_size = VAD_FRAME_SIZE
        self.energy_threshold = ENERGY_THRESHOLD
        self.zcr_threshold = ZCR_THRESHOLD
        self.spectral_threshold = SPECTRAL_CENTROID_THRESHOLD
        
        # è‡ªé€‚åº”é˜ˆå€¼
        self.background_energy = 0.001
        self.energy_history = collections.deque(maxlen=100)
        
    def extract_features(self, frame):
        """æå–éŸ³é¢‘ç‰¹å¾"""
        # 1. èƒ½é‡ç‰¹å¾
        energy = np.sum(frame ** 2) / len(frame)
        
        # 2. è¿‡é›¶ç‡ (Zero Crossing Rate)
        zcr = np.sum(np.abs(np.diff(np.sign(frame)))) / (2 * len(frame))
        
        # 3. é¢‘è°±é‡å¿ƒ (Spectral Centroid)
        fft_result = fft(frame)
        magnitude = np.abs(fft_result[:len(frame)//2])
        freqs = np.fft.fftfreq(len(frame), 1/self.sample_rate)[:len(frame)//2]
        
        if np.sum(magnitude) > 0:
            spectral_centroid = np.sum(freqs * magnitude) / np.sum(magnitude)
        else:
            spectral_centroid = 0
            
        # 4. é¢‘è°±æ»šé™ (Spectral Rolloff)
        cumsum = np.cumsum(magnitude)
        rolloff_threshold = 0.85 * cumsum[-1]
        rolloff_idx = np.where(cumsum >= rolloff_threshold)[0]
        spectral_rolloff = freqs[rolloff_idx[0]] if len(rolloff_idx) > 0 else 0
        
        return {
            'energy': energy,
            'zcr': zcr,
            'spectral_centroid': spectral_centroid,
            'spectral_rolloff': spectral_rolloff
        }
    
    def update_background_noise(self, energy):
        """æ›´æ–°èƒŒæ™¯å™ªå£°ä¼°è®¡"""
        self.energy_history.append(energy)
        if len(self.energy_history) >= 10:
            # ä½¿ç”¨èƒ½é‡å†å²çš„ä¸‹å››åˆ†ä½æ•°ä½œä¸ºèƒŒæ™¯å™ªå£°ä¼°è®¡
            self.background_energy = np.percentile(list(self.energy_history), 25)
    
    def is_speech(self, frame):
        """åˆ¤æ–­æ˜¯å¦ä¸ºè¯­éŸ³"""
        features = self.extract_features(frame)
        
        # æ›´æ–°èƒŒæ™¯å™ªå£°
        self.update_background_noise(features['energy'])
        
        # è‡ªé€‚åº”èƒ½é‡é˜ˆå€¼
        adaptive_energy_threshold = max(self.background_energy * 3, self.energy_threshold)
        
        # å¤šç‰¹å¾åˆ¤æ–­
        energy_check = features['energy'] > adaptive_energy_threshold
        zcr_check = features['zcr'] > self.zcr_threshold
        spectral_check = features['spectral_centroid'] > self.spectral_threshold
        
        # è‡³å°‘æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶æ‰è®¤ä¸ºæ˜¯è¯­éŸ³
        speech_indicators = sum([energy_check, zcr_check, spectral_check])
        
        return speech_indicators >= 2, features

class FunASRVADRecorder:
    """FunASR VAD å½•éŸ³ç³»ç»Ÿ"""
    
    def __init__(self):
        print(f"\n{Fore.CYAN}{Style.BRIGHT}{'='*60}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{Style.BRIGHT}      åˆå§‹åŒ– FunASR VAD å½•éŸ³ç³»ç»Ÿ{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{Style.BRIGHT}{'='*60}{Style.RESET_ALL}\n")
        
        # åˆå§‹åŒ–éŸ³é¢‘
        self.audio = pyaudio.PyAudio()
        self.check_audio_devices()
        
        # åˆå§‹åŒ– VAD å¤„ç†å™¨
        self.vad_processor = VADProcessor(RATE)
        
        # åˆå§‹åŒ– FunASR VAD (å¦‚æœå¯ç”¨)
        self.funasr_vad = None
        if FUNASR_AVAILABLE:
            self.init_funasr_vad()
        
        # å½•éŸ³çŠ¶æ€
        self.is_recording = False
        self.frames = []
        self.stop_event = threading.Event()
        self.last_activity_time = 0
        
        # è¯­éŸ³æ£€æµ‹çŠ¶æ€
        self.speech_active = False
        self.speech_start_time = None
        self.silence_start_time = None
        
        # éŸ³é¢‘ç¼“å†²åŒº
        self.audio_buffer = collections.deque(maxlen=int(RATE / CHUNK * 2))  # 2ç§’ç¼“å†²
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir = "recordings"
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"{Fore.GREEN}{Style.BRIGHT}âœ“ VAD å½•éŸ³ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}é…ç½®: æœ€å°è¯­éŸ³æ—¶é•¿ {MIN_SPEECH_DURATION}s, é™éŸ³è¶…æ—¶ {SILENCE_DURATION}s{Style.RESET_ALL}\n")
    
    def init_funasr_vad(self):
        """åˆå§‹åŒ– FunASR VAD æ¨¡å‹"""
        try:
            print(f"{Fore.YELLOW}æ­£åœ¨åŠ è½½ FunASR VAD æ¨¡å‹...{Style.RESET_ALL}")
            
            # è®¾ç½®æ¨¡å‹ç¼“å­˜
            current_dir = os.getcwd()
            os.environ['MODELSCOPE_CACHE'] = current_dir
            
            self.funasr_vad = AutoModel(
                model="damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                model_revision="v2.0.4"
            )
            print(f"{Fore.GREEN}âœ“ FunASR VAD æ¨¡å‹åŠ è½½æˆåŠŸ{Style.RESET_ALL}")
            
        except Exception as e:
            print(f"{Fore.RED}âœ— FunASR VAD æ¨¡å‹åŠ è½½å¤±è´¥: {e}{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}å°†ä½¿ç”¨å†…ç½® VAD ç®—æ³•{Style.RESET_ALL}")
            self.funasr_vad = None
    
    def check_audio_devices(self):
        """æ£€æŸ¥éŸ³é¢‘è®¾å¤‡"""
        device_count = self.audio.get_device_count()
        print(f"{Fore.BLUE}æ£€æµ‹åˆ° {device_count} ä¸ªéŸ³é¢‘è®¾å¤‡:{Style.RESET_ALL}")
        
        input_devices = []
        for i in range(device_count):
            info = self.audio.get_device_info_by_index(i)
            if info['maxInputChannels'] > 0:
                input_devices.append((i, info['name']))
                print(f"  {Fore.GREEN}[{i}]{Style.RESET_ALL} {info['name']}")
        
        if not input_devices:
            raise Exception("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„éŸ³é¢‘è¾“å…¥è®¾å¤‡")
        
        # è·å–é»˜è®¤è¾“å…¥è®¾å¤‡
        try:
            default_input = self.audio.get_default_input_device_info()
            print(f"{Fore.GREEN}âœ“ ä½¿ç”¨é»˜è®¤è¾“å…¥è®¾å¤‡: {default_input['name']}{Style.RESET_ALL}")
        except Exception as e:
            print(f"{Fore.YELLOW}âš  è·å–é»˜è®¤è®¾å¤‡å¤±è´¥: {e}{Style.RESET_ALL}")
    
    def funasr_vad_detect(self, audio_data):
        """ä½¿ç”¨ FunASR è¿›è¡Œ VAD æ£€æµ‹"""
        if self.funasr_vad is None:
            return None
        
        try:
            # è½¬æ¢ä¸º FunASR æ‰€éœ€çš„æ ¼å¼
            result = self.funasr_vad.generate(input=audio_data)
            
            if result and len(result) > 0:
                # è§£æ VAD ç»“æœ
                vad_segments = result[0].get('value', [])
                return len(vad_segments) > 0  # å¦‚æœæœ‰è¯­éŸ³æ®µï¼Œè¿”å› True
            
        except Exception as e:
            print(f"{Fore.RED}FunASR VAD æ£€æµ‹é”™è¯¯: {e}{Style.RESET_ALL}")
        
        return None
    
    def process_audio_chunk(self, audio_data):
        """å¤„ç†éŸ³é¢‘å—"""
        # è½¬æ¢ä¸º numpy æ•°ç»„
        audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self.audio_buffer.append(audio_array)
        
        # ä½¿ç”¨å†…ç½® VAD
        is_speech_builtin, features = self.vad_processor.is_speech(audio_array)
        
        # ä½¿ç”¨ FunASR VAD (å¦‚æœå¯ç”¨)
        is_speech_funasr = None
        if self.funasr_vad and len(self.audio_buffer) >= 10:  # ç§¯ç´¯è¶³å¤Ÿçš„éŸ³é¢‘å†æ£€æµ‹
            buffer_audio = np.concatenate(list(self.audio_buffer))
            is_speech_funasr = self.funasr_vad_detect(buffer_audio)
        
        # ç»¼åˆåˆ¤æ–­
        if is_speech_funasr is not None:
            is_speech = is_speech_funasr
            vad_method = "FunASR"
        else:
            is_speech = is_speech_builtin
            vad_method = "Built-in"
        
        return is_speech, vad_method, features
    
    def audio_callback(self, in_data, frame_count, time_info, status):
        """éŸ³é¢‘æµå›è°ƒå‡½æ•°"""
        current_time = time.time()
        
        # å¤„ç†éŸ³é¢‘
        is_speech, vad_method, features = self.process_audio_chunk(in_data)
        
        # è¯­éŸ³æ´»åŠ¨çŠ¶æ€æœº
        if is_speech:
            if not self.speech_active:
                # è¯­éŸ³å¼€å§‹
                self.speech_active = True
                self.speech_start_time = current_time
                self.silence_start_time = None
                print(f"{Fore.YELLOW}ğŸ¤ æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹ ({vad_method}) - èƒ½é‡: {features['energy']:.4f}{Style.RESET_ALL}")
            
            # æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´
            self.last_activity_time = current_time
            
        else:
            if self.speech_active:
                if self.silence_start_time is None:
                    self.silence_start_time = current_time
                
                # æ£€æŸ¥æ˜¯å¦é™éŸ³å¤ªä¹…
                silence_duration = current_time - self.silence_start_time
                if silence_duration >= SILENCE_DURATION:
                    # è¯­éŸ³ç»“æŸ
                    speech_duration = current_time - self.speech_start_time
                    print(f"{Fore.CYAN}ğŸ”‡ è¯­éŸ³ç»“æŸ - æŒç»­æ—¶é•¿: {speech_duration:.2f}s{Style.RESET_ALL}")
                    
                    if speech_duration >= MIN_SPEECH_DURATION:
                        if self.is_recording:
                            self.stop_recording()
                        else:
                            print(f"{Fore.GREEN}âœ“ æ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³æ®µ ({speech_duration:.2f}s)ï¼Œå¼€å§‹å½•éŸ³{Style.RESET_ALL}")
                            self.start_recording()
                    else:
                        print(f"{Fore.RED}âœ— è¯­éŸ³å¤ªçŸ­ ({speech_duration:.2f}s < {MIN_SPEECH_DURATION}s)ï¼Œå¿½ç•¥{Style.RESET_ALL}")
                    
                    self.speech_active = False
                    self.speech_start_time = None
                    self.silence_start_time = None
        
        # å¦‚æœæ­£åœ¨å½•éŸ³ï¼Œä¿å­˜éŸ³é¢‘æ•°æ®
        if self.is_recording:
            self.frames.append(in_data)
            
            # æ£€æŸ¥å½•éŸ³æ—¶é•¿ï¼Œé¿å…å½•éŸ³è¿‡é•¿
            if len(self.frames) > RATE * 30:  # æœ€é•¿30ç§’
                print(f"{Fore.YELLOW}â° å½•éŸ³æ—¶é•¿è¶…è¿‡30ç§’ï¼Œè‡ªåŠ¨åœæ­¢{Style.RESET_ALL}")
                self.stop_recording()
        
        return (in_data, pyaudio.paContinue)
    
    def start_recording(self):
        """å¼€å§‹å½•éŸ³"""
        if not self.is_recording:
            print(f"{Fore.GREEN}ğŸ”´ å¼€å§‹å½•éŸ³...{Style.RESET_ALL}")
            self.is_recording = True
            self.frames = []
            
            # æ·»åŠ ç¼“å†²åŒºä¸­çš„éŸ³é¢‘ï¼Œä¿ç•™è¯­éŸ³å¼€å§‹å‰çš„ä¸€äº›å†…å®¹
            for audio_chunk in list(self.audio_buffer)[-5:]:  # æœ€å5ä¸ªå—
                audio_bytes = (audio_chunk * 32768).astype(np.int16).tobytes()
                self.frames.append(audio_bytes)
    
    def stop_recording(self):
        """åœæ­¢å½•éŸ³å¹¶ä¿å­˜"""
        if self.is_recording:
            self.is_recording = False
            
            if len(self.frames) > 0:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = os.path.join(self.output_dir, f"vad_recording_{timestamp}.wav")
                self.save_audio(filename)
                
                # æ˜¾ç¤ºå½•éŸ³ä¿¡æ¯
                duration = len(self.frames) * CHUNK / RATE
                file_size = os.path.getsize(filename) / 1024
                print(f"{Fore.CYAN}âœ“ å½•éŸ³å·²ä¿å­˜: {filename}{Style.RESET_ALL}")
                print(f"{Fore.CYAN}ğŸ“Š æ—¶é•¿: {duration:.2f}s, å¤§å°: {file_size:.1f}KB{Style.RESET_ALL}")
            
            self.frames = []
            print(f"{Fore.YELLOW}ğŸ” ç»§ç»­ç›‘å¬è¯­éŸ³æ´»åŠ¨...{Style.RESET_ALL}")
    
    def save_audio(self, filename):
        """ä¿å­˜éŸ³é¢‘æ–‡ä»¶"""
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(self.audio.get_sample_size(FORMAT))
            wf.setframerate(RATE)
            wf.writeframes(b''.join(self.frames))
    
    def start_monitoring(self):
        """å¼€å§‹ç›‘å¬"""
        print(f"\n{Fore.CYAN}{Style.BRIGHT}{'='*60}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{Style.BRIGHT}             å¼€å§‹è¯­éŸ³æ´»åŠ¨æ£€æµ‹{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{Style.BRIGHT}{'='*60}{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}æ­£åœ¨ç›‘å¬è¯­éŸ³æ´»åŠ¨... æŒ‰ Ctrl+C åœæ­¢{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{'='*60}{Style.RESET_ALL}\n")
        
        try:
            stream = self.audio.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=RATE,
                input=True,
                frames_per_buffer=CHUNK,
                stream_callback=self.audio_callback
            )
            
            # æŒç»­ç›‘å¬
            while not self.stop_event.is_set():
                time.sleep(0.1)
                
        except Exception as e:
            print(f"{Fore.RED}éŸ³é¢‘æµé”™è¯¯: {e}{Style.RESET_ALL}")
        except KeyboardInterrupt:
            print(f"\n{Fore.YELLOW}ç”¨æˆ·åœæ­¢ç¨‹åº{Style.RESET_ALL}")
        finally:
            try:
                stream.stop_stream()
                stream.close()
            except:
                pass
            self.audio.terminate()
            
            if self.is_recording:
                self.stop_recording()
    
    def stop(self):
        """åœæ­¢ç³»ç»Ÿ"""
        self.stop_event.set()

def main():
    """ä¸»å‡½æ•°"""
    print(f"\n{Fore.CYAN}{Style.BRIGHT}{'='*80}{Style.RESET_ALL}")
    print(f"{Fore.CYAN}{Style.BRIGHT}                    FunASR VAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹ç³»ç»Ÿ{Style.RESET_ALL}")
    print(f"{Fore.CYAN}{Style.BRIGHT}{'='*80}{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}åŠŸèƒ½: å®æ—¶è¯­éŸ³æ´»åŠ¨æ£€æµ‹å’Œè‡ªåŠ¨å½•éŸ³{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}æ”¯æŒ: FunASR VAD + å†…ç½®å¤šç‰¹å¾ VAD ç®—æ³•{Style.RESET_ALL}")
    print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}\n")
    
    try:
        recorder = FunASRVADRecorder()
        recorder.start_monitoring()
    except KeyboardInterrupt:
        print(f"\n{Fore.RED}ç¨‹åºå·²åœæ­¢{Style.RESET_ALL}")
    except Exception as e:
        print(f"\n{Fore.RED}ç¨‹åºé”™è¯¯: {e}{Style.RESET_ALL}")

if __name__ == "__main__":
    main()
